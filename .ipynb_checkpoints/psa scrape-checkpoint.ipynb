{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "players = pd.read_csv('players.csv',index_col = 0)\n",
    "players = players.loc[players.pick_no <= 30].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAGE_MAX = 250\n",
    "LOTS_URL = \"https://www.psacard.com/auctionprices/GetItemLots\"\n",
    "SEARCH_URL = \"https://www.psacard.com/auctionprices/Search\"\n",
    "SET_NAME = 'Panini Prizm'\n",
    "CARD_SEARCH_CSV_FN = 'card_search.csv'\n",
    "CARD_LOTS_CSV_FN = 'card_lots.csv'\n",
    "CARD_CATEGORY_ID = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_lots(card_id):\n",
    "\n",
    "    total_sales, sales, curr_page = 1, [], 1\n",
    "    while total_sales > len(sales):\n",
    "\n",
    "        form_data = {\n",
    "            \"specID\": str(card_id),\n",
    "            \"draw\": curr_page,\n",
    "            \"start\": PAGE_MAX * (curr_page - 1),\n",
    "            \"length\": PAGE_MAX\n",
    "        }\n",
    "\n",
    "        with requests.Session() as sess:\n",
    "            response = sess.post(LOTS_URL, data=form_data)\n",
    "            response.raise_for_status()\n",
    "            lots_data = response.json()\n",
    "\n",
    "        sales += lots_data[\"data\"]\n",
    "        total_sales = lots_data['recordsTotal']\n",
    "        curr_page += 1\n",
    "\n",
    "    return sales\n",
    "\n",
    "def check_value_in_csv(file_path, target_value):\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            if target_value in line:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def scrape_search(search_term):\n",
    "    POST_DATA = {\n",
    "        'draw': 1,\n",
    "        'filterCategoryID': CARD_CATEGORY_ID, #basketball cards\n",
    "        'pageNumber': 1,\n",
    "        'pageSize': PAGE_MAX,\n",
    "        'search': search_term,\n",
    "        'pricesOnly': True,\n",
    "        'searchSequence': 1\n",
    "    }\n",
    "\n",
    "    with requests.Session() as sess:\n",
    "        response = sess.post(SEARCH_URL, data=POST_DATA)\n",
    "        response.raise_for_status()\n",
    "        search_results = response.json()\n",
    "        \n",
    "    return search_results\n",
    "\n",
    "def normalize_name(name):\n",
    "    \"\"\"Normalize a name for comparison.\"\"\"\n",
    "    name = name.lower()\n",
    "    for term in ['mr', 'mrs', 'ms', 'dr', 'jr', 'sr', '.']:\n",
    "        name = name.replace(term, '')\n",
    "    return ' '.join(name.split())  # Remove extra whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all_players_dict = {}\n",
    "\n",
    "base_url = 'https://www.basketball-reference.com/draft/NBA_'\n",
    "years = list(range(2013,2023))\n",
    "\n",
    "for year in years:\n",
    "    url = base_url + str(year) + '.html'\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'html5lib')\n",
    "    table = soup.find('tbody')\n",
    "    players = [x.text for x in table.find_all('td', attrs={'data-stat':'player'})]\n",
    "    all_players_dict[str(year)] = players\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if os.path.exists(CARD_SEARCH_CSV_FN) == False:\n",
    "    df = pd.DataFrame(columns = ['CardNumber', 'CategoryName', 'CategoryNameSEO', 'HeadingID',\n",
    "                                 'ItemNameSEO', 'LotsFound', 'SetName', 'SetNameSEO', 'SpecDescription',\n",
    "                                 'SpecID', 'SpecSubjectName', 'SportCategoryID', 'Variety',\n",
    "                                 'YearIssued'])\n",
    "    \n",
    "else:\n",
    "    df = pd.read_csv(CARD_SEARCH_CSV_FN)\n",
    "    \n",
    "for i, row in players.iterrows():\n",
    "    normalized_player_name = normalize_name(row.player_name)\n",
    "    player_match = df.loc[df.SpecSubjectName.apply(lambda x: normalize_name(x)) == normalized_player_name]\n",
    "\n",
    "    if len(player_match) == 0:\n",
    "        search_term = f'{row.draft_year} {SET_NAME} {row.player_name}'\n",
    "        search_results = scrape_search(search_term)\n",
    "        \n",
    "        df_temp = pd.DataFrame(search_results['data'])\n",
    "        df_temp = df_temp.loc[\n",
    "            (df_temp.SpecSubjectName.apply(lambda x: normalize_name(x)) == normalized_player_name) & # player name exact\n",
    "            (df_temp.SetNameSEO == SET_NAME.lower().replace(' ','-')) & # set name match\n",
    "            (df_temp.LotsFound >= 1)  # traded only\n",
    "        ] # basketball cards only filtered by search\n",
    "        # rookie cards only\n",
    "        if len(df_temp) > 0:\n",
    "            df_temp = df_temp.loc[df_temp.YearIssued == df_temp.YearIssued.astype(int).min().astype(str)]\n",
    "            df = pd.concat([df, df_temp], ignore_index=True)\n",
    "            df.to_csv(CARD_SEARCH_CSV_FN, index=False)\n",
    "\n",
    "        print(f'scraped {len(df_temp)} {SET_NAME} cards for {row.player_name} {CARD_SEARCH_CSV_FN} length is now {len(df) + len(df_temp)}')\n",
    "    time.sleep(0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = pd.read_csv(CARD_SEARCH_CSV_FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.YearIssued = df.YearIssued.astype(int)\n",
    "df['YearsFromIssue'] = 2023 - df.YearIssued\n",
    "df['TotalLotsFound'] = df.groupby('ItemNameSEO').LotsFound.transform('sum')\n",
    "df['MaxLotsFoundSingleCard'] = df.groupby('ItemNameSEO').LotsFound.transform('max')\n",
    "\n",
    "df['MaxLotsFoundSingleCardPerYear'] = df['MaxLotsFoundSingleCard']/df['YearsFromIssue']\n",
    "df['LotsFoundPerYear'] = df['TotalLotsFound']/df['YearsFromIssue']\n",
    "\n",
    "\"\"\" \n",
    "Only looking at players who's most traded prizm card sold an average of 50 times or more per year\n",
    "This most traded card will play role as the reference card for pricing \n",
    "And the threshold will serve to insure some type of accuracy of said reference price\n",
    "\"\"\"\n",
    "\n",
    "card_universe = df.loc[df.MaxLotsFoundSingleCardPerYear >= 50]\n",
    "print(card_universe.LotsFound.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if os.path.exists(CARD_LOTS_CSV_FN) == False:\n",
    "    df_lots = pd.DataFrame(columns = [['SpecID', 'AuctionItemID', 'AuctionName', 'AuctionType', 'CertNo', 'EndDate',\n",
    "                                       'GradeString', 'HasQualifier', 'ImageURL', 'IsPSADNA', 'LotNo', 'Name',\n",
    "                                       'NoGradeDescription', 'Qualifier', 'SalePrice', 'URL']])\n",
    "    df_lots.to_csv(CARD_LOTS_CSV_FN, header = True, index = False)\n",
    "else:\n",
    "    df_lots = pd.read_csv(CARD_LOTS_CSV_FN)\n",
    "\n",
    "current_db_size = len(df_lots)\n",
    "print(f'Starting Length of lots database {current_db_size}')\n",
    "\n",
    "card_universe_unscraped = card_universe[~card_universe.SpecID.isin(df_lots.SpecID)]\n",
    "    \n",
    "for i, row in card_universe_unscraped.iterrows():\n",
    "    spec_id_match = df_lots.loc[df_lots.SpecID == row.SpecID]\n",
    "    if len(spec_id_match) == 0:\n",
    "        try:\n",
    "            lot_data = scrape_lots(row.SpecID)\n",
    "            df_temp = pd.DataFrame(lot_data)\n",
    "            if 'SpecID' not in df_temp.columns:\n",
    "                df_temp.insert(loc=0, column='SpecID', value=row.SpecID)\n",
    "\n",
    "            df_lots = pd.concat([df_lots, df_temp], ignore_index=True)\n",
    "            df_lots.to_csv(CARD_LOTS_CSV_FN, index=False)\n",
    "            print(f'appending {len(df_temp)} lots found for {row.SpecSubjectName} {row.Variety} card (spec id: {row.SpecID}) TOTAL: {len(df_lots)}')\n",
    "        except:\n",
    "            print(f'failed to scrape {row.LotsFound} lots found for {row.SpecSubjectName} {row.Variety} card (spec id: {row.SpecID} ) ')\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
